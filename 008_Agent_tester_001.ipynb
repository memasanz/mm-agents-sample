{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1b2979",
   "metadata": {},
   "source": [
    "## AI Agent Leveraging Functin Apps for Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4460933b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "  \n",
    "# Load the .env file  \n",
    "load_dotenv(override=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fb0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from typing import Any, Callable, Set, Dict, List, Optional\n",
    "\n",
    "def fetch_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location: The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"location:\", location)\n",
    "\n",
    "    import requests\n",
    "\n",
    "    url = \"https://func.azurewebsites.net/api/http_trigger\"\n",
    "    params = {\"code\": \"blah\", \"location\": location}\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # Print the response\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"Response Body:\", response.text)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Define user functions\n",
    "user_functions = {fetch_weather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de701792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def find_or_create_agent(agent_list, agent_name, logger):\n",
    "    agent = None\n",
    "    if agent_list:\n",
    "        async for agent_object in agent_list:\n",
    "            if agent_object.name == agent_name:\n",
    "                agent = agent_object\n",
    "                logger.info(f\"Found agent by name '{agent_name}', ID={agent_object.id}\")\n",
    "                return agent\n",
    "    \n",
    "        if not agent:\n",
    "            logger.info(f\"Agent with name '{agent_name}' not found, creating a new agent.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede18783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:07:04,444 - INFO - Logger is now printing to the notebook!\n",
      "2025-07-03 12:07:08,144 - INFO - Application Insights connection string: InstrumentationKey=ff987441-70b7-44fc-aa34-15451599ad0e;IngestionEndpoint=https://eastus2-3.in.applicationinsights.azure.com/;LiveEndpoint=https://eastus2.livediagnostics.monitor.azure.com/;ApplicationId=82c61bb8-3b97-4b35-8985-4009623618f8\n",
      "2025-07-03 12:07:14,058 - INFO - Found agent by name 'mmx-ai-agent05', ID=asst_qBptgH0scFOlmmdLd381Ogqc\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.agents.models import FunctionTool\n",
    "from azure.ai.agents.models import FunctionTool, ToolSet, ListSortOrder\n",
    "import logging\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# Configure logger\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(\"notebook_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Clear existing handlers to avoid duplicate logs\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Create handler that outputs to notebook\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Example usage\n",
    "logger.info(\"Logger is now printing to the notebook!\")\n",
    "\n",
    "\n",
    "\n",
    "# Retrieve the project endpoint from environment variables\n",
    "project_endpoint = os.environ[\"PROJECT_ENDPOINT\"]\n",
    "\n",
    "# Initialize the AIProjectClient\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "os.environ['AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED'] = 'true'\n",
    "# Enable Azure Monitor tracing\n",
    "application_insights_connection_string = project_client.telemetry.get_connection_string()\n",
    "\n",
    "logger.info(f\"Application Insights connection string: {application_insights_connection_string}\")\n",
    "\n",
    "if not application_insights_connection_string:\n",
    "    print(\"Application Insights was not enabled for this project.\")\n",
    "    print(\"Enable it via the 'Tracing' tab in your Azure AI Foundry project page.\")\n",
    "    exit()\n",
    "    \n",
    "configure_azure_monitor(connection_string=application_insights_connection_string)\n",
    "\n",
    "agent_name = os.environ[\"AZURE_AI_AGENT_NAME\"]\n",
    "agent_list = project_client.agents.list_agents()\n",
    "\n",
    "agent = None\n",
    "if agent_list:\n",
    "    for agent_object in agent_list:\n",
    "        if agent_object.name == agent_name:\n",
    "            agent = agent_object\n",
    "\n",
    "if not agent:\n",
    "    logger.info(f\"Agent with name '{agent_name}' not found, creating a new agent.\")\n",
    "    functions = FunctionTool({fetch_weather})\n",
    "    toolset = ToolSet()\n",
    "    toolset.add(functions)\n",
    "    project_client.agents.enable_auto_function_calls(toolset)\n",
    "\n",
    "        # Create an agent with custom functions\n",
    "\n",
    "    agent = project_client.agents.create_agent(\n",
    "            model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    "            name=agent_name,\n",
    "            instructions=\"You are a helpful agent with tools to fetch weather information.\",\n",
    "            tools=functions.definitions,\n",
    "        )\n",
    "    logger.info(f\"Created agent, ID: {agent.id}\")\n",
    "else:\n",
    "    logger.info(f\"Found agent by name '{agent_name}', ID={agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1601d3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created thread, ID: thread_HG6SHWIV5SxWmKueW9HAa5rN\n",
      "Created message, ID: msg_rXvgV9fviEbdIq11yAGsjfgH\n"
     ]
    }
   ],
   "source": [
    "thread = project_client.agents.threads.create()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Create message to thread\n",
    "message = project_client.agents.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",  # Role of the message sender\n",
    "        content=\"what is the weather in London?\",  # Message content\n",
    "    )\n",
    "print(f\"Created message, ID: {message['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45aca495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run finished with status: RunStatus.COMPLETED\n",
      "mmx-ai-agent05\n",
      "asst_qBptgH0scFOlmmdLd381Ogqc\n"
     ]
    }
   ],
   "source": [
    "run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "print(agent.name)\n",
    "print(agent.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee003511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_XoaqY7rURhv6l7ZElhNR6SqM', 'object': 'thread.run', 'created_at': 1751562433, 'assistant_id': 'asst_qBptgH0scFOlmmdLd381Ogqc', 'thread_id': 'thread_HG6SHWIV5SxWmKueW9HAa5rN', 'status': 'completed', 'started_at': 1751562436, 'expires_at': None, 'cancelled_at': None, 'failed_at': None, 'completed_at': 1751562437, 'required_action': None, 'last_error': None, 'model': 'gpt-4.1', 'instructions': 'You are a helpful agent with tools to fetch weather information.', 'tools': [{'type': 'function', 'function': {'name': 'fetch_weather', 'description': 'Fetches the weather information for the specified location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location to fetch weather for.'}}, 'required': ['location']}, 'strict': False}}], 'tool_resources': {}, 'metadata': {}, 'temperature': 1.0, 'top_p': 1.0, 'max_completion_tokens': None, 'max_prompt_tokens': None, 'truncation_strategy': {'type': 'auto', 'last_messages': None}, 'incomplete_details': None, 'usage': {'prompt_tokens': 612, 'completion_tokens': 54, 'total_tokens': 666, 'prompt_token_details': {'cached_tokens': 0}}, 'response_format': 'auto', 'tool_choice': 'auto', 'parallel_tool_calls': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb63c45",
   "metadata": {},
   "source": [
    "## View your Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1103f82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: MessageRole.AGENT, Content: [{'type': 'text', 'text': {'value': \"I'm unable to retrieve the weather for London at this moment due to a system issue. Would you like to try again, or do you want information on typical weather conditions in London instead?\", 'annotations': []}}]\n",
      "Role: MessageRole.USER, Content: [{'type': 'text', 'text': {'value': 'what is the weather in London?', 'annotations': []}}]\n"
     ]
    }
   ],
   "source": [
    "if run.status == \"failed\":\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "    \n",
    "# Fetch and log all messages\n",
    "messages = project_client.agents.messages.list(thread_id=thread.id)\n",
    "for message in messages:\n",
    "    print(f\"Role: {message.role}, Content: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c23f3",
   "metadata": {},
   "source": [
    "## View your Tool Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68ce374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tool calls:\n",
      "{'id': 'call_zZPCjezldSBH8BkqVKfq5ShU', 'type': 'function', 'function': {'name': 'fetch_weather', 'arguments': '{\"location\":\"London\"}', 'output': '{\"error\": \"Error executing function \\'fetch_weather\\': Function \\'fetch_weather\\' not found. Provide this function to `enable_auto_function_calls` function call.\"}'}}\n",
      "    Tool Call ID: call_zZPCjezldSBH8BkqVKfq5ShU\n",
      "    Type: function\n",
      "    Function Name: fetch_weather\n",
      "    Function Arguments: {\"location\":\"London\"}\n"
     ]
    }
   ],
   "source": [
    "run_steps = project_client.agents.run_steps.list(thread_id=thread.id, run_id=run.id)\n",
    "for step in run_steps:\n",
    "        # print(f\"Step {step['id']} status: {step['status']}\")\n",
    "        step_details = step.get(\"step_details\", {})\n",
    "        tool_calls = step_details.get(\"tool_calls\", [])\n",
    "\n",
    "        if tool_calls:\n",
    "            print(\"  Tool calls:\")\n",
    "            for call in tool_calls:\n",
    "                print(call)\n",
    "                print(f\"    Tool Call ID: {call.get('id')}\")\n",
    "                print(f\"    Type: {call.get('type')}\")\n",
    "                type = call.get(\"type\")\n",
    "                if type == \"function\":\n",
    "                    print(f\"    Function Name: {call.get('function', {}).get('name')}\")\n",
    "                    print(f\"    Function Arguments: {call.get('function', {}).get('arguments')}\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4046ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator, AzureOpenAIModelConfiguration\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089cddc",
   "metadata": {},
   "source": [
    "## Check Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d68581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessageRole.AGENT\n",
      "MessageRole.USER\n",
      "checking coherence\n",
      "2025-07-03 12:07:31 -0500   27968 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. NotFoundError: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 377, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<45 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n"
     ]
    },
    {
     "ename": "WrappedOpenAIError",
     "evalue": "(UserError) OpenAI API hits NotFoundError: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py:377\u001b[39m, in \u001b[36mAsyncPrompty._send_with_retries\u001b[39m\u001b[34m(self, api_client, params, timeout, max_retries, max_entity_retries)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(delay)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(**params)\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2027\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m         {\n\u001b[32m   2032\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m         },\n\u001b[32m   2064\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m     ),\n\u001b[32m   2068\u001b[39m     options=make_request_options(\n\u001b[32m   2069\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m     ),\n\u001b[32m   2071\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1748\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1745\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1746\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1747\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1555\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1554\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1557\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWrappedOpenAIError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m user_message \u001b[38;5;129;01mand\u001b[39;00m agent_message:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mchecking coherence\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     coherence_score = \u001b[43mcoherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCoherence score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoherence_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_coherence\\_coherence.py:136\u001b[39m, in \u001b[36mCoherenceEvaluator.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=docstring-missing-param\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    118\u001b[39m     *args,\n\u001b[32m    119\u001b[39m     **kwargs,\n\u001b[32m    120\u001b[39m ):\n\u001b[32m    121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Evaluate coherence. Accepts either a query and response for a single evaluation,\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[33;03m    or a conversation for a potentially multi-turn evaluation. If the conversation has more than one pair of\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m    turns, the evaluator will aggregate the results of each turn.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \u001b[33;03m    :rtype: Union[Dict[str, float], Dict[str, Union[float, Dict[str, List[float]]]]]\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_common\\_base_eval.py:136\u001b[39m, in \u001b[36mEvaluatorBase.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=docstring-missing-param\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    123\u001b[39m     *args,\n\u001b[32m    124\u001b[39m     **kwargs,\n\u001b[32m    125\u001b[39m ) -> Union[DoEvalResult[T_EvalValue], AggregateResult[T_EvalValue]]:\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Evaluate a given input. This method serves as a wrapper and is meant to be overridden by child classes for\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    one main reason - to overwrite the method headers and docstring to include additional inputs as needed.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m    The actual behavior of this function shouldn't change beyond adding more inputs to the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \u001b[33;03m    :rtype: Union[DoEvalResult[T_EvalValue], AggregateResult[T_EvalValue]]\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masync_run_allowing_running_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_async_evaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\promptflow\\_utils\\async_utils.py:94\u001b[39m, in \u001b[36masync_run_allowing_running_loop\u001b[39m\u001b[34m(async_func, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_running_loop():\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutorWithContext() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio.run(_invoke_async_with_sigint_handler(async_func, *args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\promptflow\\_utils\\async_utils.py:94\u001b[39m, in \u001b[36masync_run_allowing_running_loop.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_running_loop():\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutorWithContext() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m executor.submit(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m).result()\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio.run(_invoke_async_with_sigint_handler(async_func, *args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\asyncio\\runners.py:195\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\asyncio\\runners.py:118\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(self, coro, context)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._interrupt_count = \u001b[32m0\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.CancelledError:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._interrupt_count > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\asyncio\\base_events.py:725\u001b[39m, in \u001b[36mBaseEventLoop.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future.done():\n\u001b[32m    723\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_common\\_base_eval.py:528\u001b[39m, in \u001b[36mAsyncEvaluatorBase.__call__\u001b[39m\u001b[34m(self, query, response, context, conversation, ground_truth, tool_calls, tool_definitions, messages, retrieval_ground_truth, retrieved_documents, **kwargs)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retrieved_documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    526\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mretrieved_documents\u001b[39m\u001b[33m\"\u001b[39m] = retrieved_documents\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._real_call(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_common\\_base_eval.py:408\u001b[39m, in \u001b[36mEvaluatorBase._real_call\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# Evaluate all inputs.\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eval_input \u001b[38;5;129;01min\u001b[39;00m eval_input_list:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._do_eval(eval_input)\n\u001b[32m    409\u001b[39m     \u001b[38;5;66;03m# logic to determine threshold pass/fail\u001b[39;00m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_common\\_base_prompty_eval.py:115\u001b[39m, in \u001b[36mPromptyEvaluatorBase._do_eval\u001b[39m\u001b[34m(self, eval_input)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eval_input \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eval_input:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    109\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[33mOnly text conversation inputs are supported.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    110\u001b[39m         internal_message=\u001b[33m\"\u001b[39m\u001b[33mOnly text conversation inputs are supported.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    113\u001b[39m         target=ErrorTarget.CONVERSATION,\n\u001b[32m    114\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m llm_output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._flow(timeout=\u001b[38;5;28mself\u001b[39m._LLM_CALL_TIMEOUT, **eval_input)\n\u001b[32m    117\u001b[39m score = math.nan\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m llm_output:\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# Parse out score and reason from evaluators known to possess them.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py:317\u001b[39m, in \u001b[36mAsyncPrompty.__call__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError(\n\u001b[32m    314\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(connection).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not a supported connection type.\u001b[39m\u001b[33m\"\u001b[39m, target=ErrorTarget.EVAL_RUN\n\u001b[32m    315\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m response: OpenAIChatResponseType = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_with_retries(\n\u001b[32m    318\u001b[39m     api_client=api_client,\n\u001b[32m    319\u001b[39m     params=params,\n\u001b[32m    320\u001b[39m     timeout=timeout,\n\u001b[32m    321\u001b[39m )\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m format_llm_response(\n\u001b[32m    324\u001b[39m     response=response,\n\u001b[32m    325\u001b[39m     is_first_choice=\u001b[38;5;28mself\u001b[39m._data.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m\"\u001b[39m).lower() == \u001b[33m\"\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    326\u001b[39m     response_format=params.get(\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m, {}),\n\u001b[32m    327\u001b[39m     outputs=\u001b[38;5;28mself\u001b[39m._outputs,\n\u001b[32m    328\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\memasanz\\repos\\ai-foundry-agent-func\\.venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py:405\u001b[39m, in \u001b[36mAsyncPrompty._send_with_retries\u001b[39m\u001b[34m(self, api_client, params, timeout, max_retries, max_entity_retries)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger.exception(\n\u001b[32m    398\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m request failed. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    399\u001b[39m         retry,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m         \u001b[38;5;28mstr\u001b[39m(error),\n\u001b[32m    404\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WrappedOpenAIError(error=error) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    407\u001b[39m retry += \u001b[32m1\u001b[39m\n",
      "\u001b[31mWrappedOpenAIError\u001b[39m: (UserError) OpenAI API hits NotFoundError: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]"
     ]
    }
   ],
   "source": [
    "# Extract user and agent messages for coherence evaluation\n",
    "coherence = CoherenceEvaluator(model_config=model_config, threshold=3)\n",
    "user_message = None\n",
    "agent_message = None\n",
    "messages = project_client.agents.messages.list(thread_id=thread.id)\n",
    "for message in messages:\n",
    "    print(message.role)\n",
    "    if str(message.role) == \"MessageRole.USER\" and user_message is None:\n",
    "        user_message = message.content\n",
    "    elif str(message.role) in (\"MessageRole.AGENT\") and agent_message is None:\n",
    "        agent_message = message.content\n",
    "\n",
    "if user_message and agent_message:\n",
    "    print(\"checking coherence\")\n",
    "    coherence_score = coherence(query=user_message, response=agent_message)\n",
    "    print(f\"Coherence score: {coherence_score}\")\n",
    "else:\n",
    "    print(\"Could not find both user and agent messages for coherence evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73329b4a",
   "metadata": {},
   "source": [
    "## Check Intent Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import IntentResolutionEvaluator\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config, threshold=3)\n",
    "intent_resolution(\n",
    "    query=user_message,\n",
    "    response=agent_message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44fdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176145c8",
   "metadata": {},
   "source": [
    "## QA Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "qa_eval = QAEvaluator(model_config=model_config, threshold=3)\n",
    "qa_eval(\n",
    "    query=\"Where was Marie Curie born?\", \n",
    "    context=\"Background: 1. Marie Curie was a chemist. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist.\",\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7986f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
